{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import plotly.subplots as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Step 1: Import the data\n",
    "file_path = \"cw1data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result of the code in cell 3 it was observed that the dataset has 135 rows and 14 columns. Each column contains 135 non-null values, meaning there are no missing values. Most columns (13 out of 14) are of type float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Display basic info\n",
    "print(\"Dataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Distribution Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code provides a comprehensive view of the distribution of numerical features in the dataset, helping us uncover hidden patterns and characteristics that might influence future analysis or model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select only numerical columns\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Create subplots with 3 columns per row\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "num_features = len(numeric_cols)\n",
    "cols = 3  # Number of columns per row\n",
    "rows = (num_features // cols) + (num_features % cols > 0)\n",
    "\n",
    "fig = make_subplots(rows=rows, cols=cols, subplot_titles=numeric_cols)\n",
    "\n",
    "# Add histograms for each feature\n",
    "for i, feature in enumerate(numeric_cols):\n",
    "    row = (i // cols) + 1\n",
    "    col = (i % cols) + 1\n",
    "    fig.add_trace(\n",
    "        go.Histogram(x=df[feature], nbinsx=20, name=feature, \n",
    "                     marker=dict(color=px.colors.qualitative.Prism[i % len(px.colors.qualitative.Prism)])), \n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Feature Distributions\",\n",
    "    height=800, width=1200, \n",
    "    showlegend=False,\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram grid shows the distribution of multiple features, highlighting their spread, central tendencies, and variability. Some features, like `x5` and `x6`, follow a normal distribution, while others, such as `x1` and `x2`, are skewed. Features like `y`, `x3`, and `x12` have multiple peaks, indicating variability. Understanding these distributions helps in detecting skewness, outliers, and feature importance for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is essential for understanding how features in the dataset relate to one another by visualizing their correlation matrix as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Create an interactive correlation heatmap\n",
    "fig_heatmap = go.Figure(data=go.Heatmap(\n",
    "    z=corr_matrix.values,\n",
    "    x=corr_matrix.columns,\n",
    "    y=corr_matrix.index,\n",
    "    colorscale='RdBu',  \n",
    "    colorbar=dict(title=\"Correlation\"),\n",
    "    zmin=-1, zmax=1,\n",
    "    text=corr_matrix.round(2).astype(str).values,  \n",
    "    texttemplate=\"%{text}\",  \n",
    "    hoverinfo=\"z+text\"\n",
    "))\n",
    "\n",
    "fig_heatmap.update_layout(\n",
    "    title=\"Feature Correlation Heatmap\",\n",
    "    xaxis_title=\"Features\",\n",
    "    yaxis_title=\"Features\",\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Feature Correlation Heatmap reveals important relationships between variables in the dataset. Strongly correlated features, such as x4, x5, x6, x7, and x8, suggest redundancy, which could lead to multicollinearity issues. The negative correlations between x3, x4, x5, x6, x7, and x8 with y indicate that as these features increase, y tends to decrease, highlighting an inverse relationship. On the other hand, x1 and x2 show a positive correlation with y, with x2 having the strongest influence, making it a key feature for prediction. Meanwhile, features like x10, x11, x12, and x13 exhibit weaker correlations, suggesting they may have a more independent role in the dataset. These insights are essential for feature selection, model optimization, and avoiding redundant information in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
